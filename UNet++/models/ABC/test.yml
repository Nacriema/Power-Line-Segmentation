dataset:
    name: TTPLA_Test
    restricted_labels: [1]

    # If batch size is 1, we can consider 4K resolution [3840, 2160] or [2560, 1440], [1920, 1080], [1280, 720], [640, 360]
    img_size: [640, 360]

    # Normalize is trigger the instance norm the data when batch size = 1
    # normalize: True   # Not used anymore !!!

    # Data distribution is use for recenter the dataset (recenter image instance) before training
    # If not given the data_distribution means that we don't want to recenter the image
    # One can try is ImageNet data distribution: [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]
    data_distribution:
        mean: [0.4616, 0.4506, 0.4154]
        std: [0.2368, 0.2339, 0.2415]

    data_augmentation: True

    # Data augmentations:
    blur_radius_range: [0.2, 2]
    brightness_factor_range: [0.9, 1.1]
    contrast_factor_range: [0.5, 2]
    rotation_angle_range: [-10, 10]
    sampling_ratio_range: [1, 1]  # This make the input size not variant
    sampling_max_nb_pixels: 3.5e+6

    # Transposition weights bellow used when batch_size is 1
    # transposition_weights: [0.25, 0.25, 0.25, 0.25]

model:
    name: resnet34-unet++   # This name must be fixed
    norm_layer:
        name: 'batch_norm'
        momentum: 0.1
        affine: True
        track_running_stats: True
        # This params need for the group norm instead
        num_groups: 32

    # This will take 2 effect: 1. When training, 2. When testing
    pretrained_encoder: True


# Training configs part
training:
    cudnn_benchmark: False   # Set it to True when the input for the training process is fixed
    batch_size: 1
    n_workers: 0

    # Add patience field (pytorchtools)
    patience: 5

    optimizer:
        name: adam
        lr: 1.0e-3
        weight_decay: 1.0e-6
    scheduler:
        name: multi_step
        gamma: 0.5
        # List of EPOCH indices, must be increasing
        milestones: [30, 60, 80]   # 30, 60, 80
        update_range: epoch
    loss:
        name: cross_entropy
        weight: [1.2184e-09, 5.5866e-08]
    n_epoches: 2
    # This is in iteration unit (number_of_iterations = number_of_batches that has beeb processed)
    # Ex: Test dataset train has 10 sample, batch_size = 1 and epochs = 2 => 20 iterations
    # Train_stat_interval = 5 -> Check the training loss: 20 / 5 = 4 times. Check OK
    train_stat_interval: 5

    # Same with the val_stat_interval, I set it to smaller than train_stat_interval -> Check it frequently
    # 20 / 2 = 10 times
    val_stat_interval: 5
    pretrained:
    # Resume use to continue training model, pass it with the model TAG folder name: models/TAG, if not given then train from beginning
    resume: